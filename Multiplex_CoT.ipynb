{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Environment Dependent on Installation"
      ],
      "metadata": {
        "id": "kILo7yIvLt2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install llama-index\n",
        "!pip install llama-index-llms-openai"
      ],
      "metadata": {
        "id": "HoVpWFQAMsQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refining the Concept of an Agent"
      ],
      "metadata": {
        "id": "RWWPWAOWML0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings, ServiceContext\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.agent import ReActAgent\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "prompt_template = '''\n",
        "# Multiple-CoT\n",
        "\n",
        "## Role\n",
        "\n",
        "You are an expert AI assistant capable of gradually explaining the reasoning process.\n",
        "\n",
        "## First Think step\n",
        "\n",
        "\n",
        "For each step, provide a title that describes what you did in that step, along with the corresponding content.\n",
        "Decide whether another step is needed or if you are ready to give the final answer.\n",
        "To improve instruction compliance, emphasize the importance of the instructions through `Markdown` syntax, including a set of tips and best practices:\n",
        "1. Use as many **reasoning steps** as possible. At least 3 steps.\n",
        "2. Be aware of your limitations as an AI and what you can and cannot do.\n",
        "3. Include exploration of alternative answers. Consider that you might be wrong and where the error might be if your reasoning is incorrect.\n",
        "4. When you say you are rechecking, actually recheck and use another method. Don't just say you are rechecking.\n",
        "5. Use at least 3 methods to arrive at the answer.\n",
        "6. Use best practices.\n",
        "\n",
        "## Second Think step\n",
        "\n",
        "\n",
        "For each step mentioned in the previous text, initiate a small sub-step within each step to verify its correctness. After completing each step, start a `reviewer CoT` to review the current step from different perspectives.\n",
        "1. Use as many **reasoning steps** as possible. At least three steps.\n",
        "2. Be aware of your limitations as an AI and what you can and cannot do.\n",
        "3. Include exploring alternative answers. Consider that you might be wrong and where the error might be if your reasoning is incorrect.'''\n",
        "\n",
        "from llama_index.core.agent import ReActAgent\n",
        "\n",
        "agent = ReActAgent.from_tools(\n",
        "    tools=[],\n",
        "    verbose=True,\n",
        "    system_prompt=prompt_template\n",
        ")\n"
      ],
      "metadata": {
        "id": "-AEhDXITMYkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GSM-8K Evaluation"
      ],
      "metadata": {
        "id": "N-SqtJFKPE3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import os\n",
        "\n",
        "PROMPT_1 = \"\"\"\n",
        "# Multiple-CoT\n",
        "\n",
        "## Role\n",
        "\n",
        "You are an expert AI assistant capable of gradually explaining the reasoning process.\n",
        "\n",
        "## First Think step\n",
        "\n",
        "\n",
        "For each step, provide a title that describes what you did in that step, along with the corresponding content.\n",
        "Decide whether another step is needed or if you are ready to give the final answer.\n",
        "To improve instruction compliance, emphasize the importance of the instructions through `Markdown` syntax, including a set of tips and best practices:\n",
        "1. Use as many **reasoning steps** as possible. At least 3 steps.\n",
        "2. Be aware of your limitations as an AI and what you can and cannot do.\n",
        "3. Include exploration of alternative answers. Consider that you might be wrong and where the error might be if your reasoning is incorrect.\n",
        "4. When you say you are rechecking, actually recheck and use another method. Don't just say you are rechecking.\n",
        "5. Use at least 3 methods to arrive at the answer.\n",
        "6. Use best practices.\n",
        "\n",
        "## Second Think step\n",
        "\n",
        "\n",
        "For each step mentioned in the previous text, initiate a small sub-step within each step to verify its correctness. After completing each step, start a `reviewer CoT` to review the current step from different perspectives.\n",
        "1. Use as many **reasoning steps** as possible. At least three steps.\n",
        "2. Be aware of your limitations as an AI and what you can and cannot do.\n",
        "3. Include exploring alternative answers. Consider that you might be wrong and where the error might be if your reasoning is incorrect.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_2 = \"\"\"Hi there\"\"\"\n",
        "\n",
        "def create_answer_extraction_agent(llm):\n",
        "\n",
        "    def extract_final_number(text: str) -> str:\n",
        "        if \"####\" in text:\n",
        "            match = re.search(r\"####\\s*(-?\\d*\\.?\\d+)\", text)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "\n",
        "        numbers = re.findall(r\"(-?\\d*\\.?\\d+)\", text)\n",
        "        if numbers:\n",
        "            return numbers[-1]\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    extract_tool = FunctionTool.from_defaults(\n",
        "        fn=extract_final_number,\n",
        "        name=\"extract_final_number\",\n",
        "        description=\"Extract the final numerical answer from the text. Only extract the numbers and do not output any additional explanatory text. Output only Arabic numerals.\"\n",
        "    )\n",
        "\n",
        "    agent = ReActAgent.from_tools(\n",
        "        tools=[extract_tool],\n",
        "        llm=llm,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    return agent\n",
        "\n",
        "def load_gsm8k_dataset(path=\"test.jsonl\", num_samples=50):\n",
        "    questions = []\n",
        "    answers = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            questions.append(data['question'])\n",
        "            answer = data['answer'].split('####')[-1].strip()\n",
        "            answers.append(answer)\n",
        "            if len(questions) >= num_samples:\n",
        "                break\n",
        "    return questions[:5], answers[:5]\n",
        "\n",
        "def evaluate_prompt(system_prompt, questions, answers):\n",
        "    llm = OpenAI(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        api_key=API_KEY,\n",
        "        api_base=API_BASE,\n",
        "        system_prompt=system_prompt\n",
        "    )\n",
        "\n",
        "    agent = create_answer_extraction_agent(llm)\n",
        "\n",
        "    correct = 0\n",
        "    responses = []\n",
        "    extracted_answers = []\n",
        "\n",
        "    for q, a in tqdm(zip(questions, answers), total=len(questions)):\n",
        "        try:\n",
        "            response = llm.complete(q)\n",
        "            responses.append(response.text)\n",
        "\n",
        "            agent_response = agent.chat(\n",
        "                f\"Extract the final numerical answer from the text. Only extract the numbers and do not output any additional explanatory text. Output only Arabic numerals.\\n{response.text}\"\n",
        "            )\n",
        "            extracted_answer = agent_response.response.strip()\n",
        "            extracted_answers.append(extracted_answer)\n",
        "\n",
        "            if str(extracted_answer).strip() == str(a).strip():\n",
        "                correct += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {e}\")\n",
        "            responses.append(\"Error\")\n",
        "            extracted_answers.append(\"Error\")\n",
        "\n",
        "    accuracy = correct / len(questions)\n",
        "    return accuracy, responses, extracted_answers\n",
        "\n",
        "def main():\n",
        "    questions, answers = load_gsm8k_dataset(num_samples=50)\n",
        "\n",
        "    print(\"Testing Prompt 1...\")\n",
        "    accuracy1, responses1, extracted1 = evaluate_prompt(PROMPT_1, questions, answers)\n",
        "\n",
        "    print(\"Testing Prompt 2...\")\n",
        "    accuracy2, responses2, extracted2 = evaluate_prompt(PROMPT_2, questions, answers)\n",
        "\n",
        "    print(\"\\nResults:\")\n",
        "    print(f\"Prompt 1 Accuracy: {accuracy1:.2%}\")\n",
        "    print(f\"Prompt 2 Accuracy: {accuracy2:.2%}\")\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'Question': questions,\n",
        "        'Correct Answer': answers,\n",
        "        'Prompt 1 Response': responses1,\n",
        "        'Prompt 1 Extracted': extracted1,\n",
        "        'Prompt 2 Response': responses2,\n",
        "        'Prompt 2 Extracted': extracted2,\n",
        "        'Prompt 1 Correct': [e == a for e, a in zip(extracted1, answers)],\n",
        "        'Prompt 2 Correct': [e == a for e, a in zip(extracted2, answers)]\n",
        "    })\n",
        "\n",
        "    results_df.to_csv('prompt_comparison_results.csv', index=False)\n",
        "\n",
        "    print(\"\\nDetailed Statistics:\")\n",
        "    print(\"Prompt 1:\")\n",
        "    print(f\"Total Correct: {sum(results_df['Prompt 1 Correct'])}\")\n",
        "    print(f\"Total Questions: {len(results_df)}\")\n",
        "    print(\"\\nPrompt 2:\")\n",
        "    print(f\"Total Correct: {sum(results_df['Prompt 2 Correct'])}\")\n",
        "    print(f\"Total Questions: {len(results_df)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "oh8OhgwuPHX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}